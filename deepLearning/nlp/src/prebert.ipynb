{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from util import load_data\n",
    "data_path='../data/sst2_shuffled.tsv.1'\n",
    "train_data,test_data,categories=load_data.load_sentence_polarity(data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里新的模型transformer\n",
    "# 先确定pre train模型的名称，所确定的tokenize\n",
    "# 加载预训练模型，因为这里是英文数据集，需要用在英文上的预训练模型：bert-base-uncased\n",
    "# uncased指该预训练模型对应的词表不区分字母的大小写\n",
    "# 详情可了解：https://huggingface.co/bert-base-uncased\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写好制作数据集的方式，先定义dataset、后面定义dataloader\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset=dataset\n",
    "        self.data_size=len(dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "def coffate_fn(examples):\n",
    "    inputs,targets=[],[]\n",
    "    for polar,sent in examples:\n",
    "        inputs.append(sent)\n",
    "        targets.append(int(polar))\n",
    "    # 这里的tokenizer是后面提供好pretrain model之后的API\n",
    "    inputs = tokenizer(inputs,\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\",\n",
    "                       max_length=512)\n",
    "    targets = torch.tensor(targets)\n",
    "    return inputs,targets\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "# 加载预训练模型对应的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "batch_size=32\n",
    "train_dataset=BertDataset(train_data)\n",
    "test_dataset=BertDataset(test_data)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,collate_fn=coffate_fn,shuffle=True)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=batch_size,collate_fn=coffate_fn,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之后是定义模型的名称\n",
    "class BertSST2Model(nn.Module):\n",
    "    def __init__(self,class_size,pretrained_model_name=pretrained_model_name) -> None:\n",
    "        super(BertSST2Model,self).__init__()\n",
    "        # 在这里编程\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        前向推理的过程\n",
    "        inputs 处理好的数据 shape=batchsize*max_len\n",
    "\n",
    "        \"\"\"\n",
    "        # 在这里编程\n",
    "        pass\n",
    "\n",
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=64\n",
    "num_epoch=200\n",
    "check_step=20\n",
    "learning_rate=1e-5\n",
    "model=BertSST2Model(class_size=2)\n",
    "model.to(device)\n",
    "optimizer=Adam(model.parameters(),learning_rate)\n",
    "celoss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 记录当前训练时间，用以记录日志和存储\n",
    "timestamp = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.train()\n",
    "for epoch in range(1,num_epoch+1):\n",
    "    # 在这里编程\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录checkpoint,之后load最新的参数\n",
    "# model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "model.eval()\n",
    "test='why you are so nerd'\n",
    "test=tokenizer(test,padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512)\n",
    "test.to(device)\n",
    "if model(test).argmax(-1).item()==1:\n",
    "    print('This is a negative sentence')\n",
    "else:\n",
    "    print('This is a positive sentence')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ca69f3d02436474f504e5bc8aa3d57c6bfabac6fe00c9a130f4089cd6bf168"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
